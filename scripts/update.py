import os
import json
import requests
from datetime import datetime, timedelta, timezone

# ============================================================== #
# ğŸ§© è¨­å®š
# ============================================================== #

CHANNEL_KEYS = {
    "UCgbQLx3kC5_i-0J_empIsxA": os.environ.get("YOUTUBE_KEY_MORE"),
    "UCSxorXiovSSaafcDp_JJAjg": os.environ.get("YOUTUBE_KEY_APOLLO"),
    "UCyBaf1pv1dO_GnkFBg1twLA": os.environ.get("YOUTUBE_KEY_MAHO"),
    "UCsy_jJ1qOyhr7wA4iKiq4Iw": os.environ.get("YOUTUBE_KEY_BIBI"),
    "UCrw103c53EKupQnNQGC4Gyw": os.environ.get("YOUTUBE_KEY_RAMU"),
    "UC_kfGHWj4_7wbG3dBLnscRA": os.environ.get("YOUTUBE_KEY_PICO"),
    "UCPFrZbMFbZ47YO7OBnte_-Q": os.environ.get("YOUTUBE_KEY_SOCHIMARU"),
}

DATA_PATH = "data/streams.json"
BACKUP_PATH = "data/streams_backup.json"

DAYS_LIMIT = 30
CUTOFF = datetime.now(timezone.utc) - timedelta(days=DAYS_LIMIT)

# ============================================================== #
# ğŸ§° ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================== #

def load_cache():
    if not os.path.exists(DATA_PATH):
        return {"live": [], "upcoming": [], "completed": [], "uploaded": [], "freechat": []}
    try:
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        return {"live": [], "upcoming": [], "completed": [], "uploaded": [], "freechat": []}

def backup_current():
    if os.path.exists(DATA_PATH):
        os.makedirs("data", exist_ok=True)
        with open(DATA_PATH, "r", encoding="utf-8") as src, open(BACKUP_PATH, "w", encoding="utf-8") as dst:
            dst.write(src.read())
        print(f"ğŸ—‚ï¸ Backup created: {BACKUP_PATH}")

# ============================================================== #
# ğŸ¬ è©³ç´°æƒ…å ±å–å¾—
# ============================================================== #

def fetch_details(video_ids, key):
    if not video_ids:
        return []

    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {
        "part": "snippet,liveStreamingDetails,contentDetails",
        "id": ",".join(video_ids),
        "key": key,
    }
    res = requests.get(url, params=params)
    if res.status_code == 403:
        print("âš ï¸ 403 Forbidden during details fetch")
        return []
    res.raise_for_status()

    videos = []
    for item in res.json().get("items", []):
        snippet = item.get("snippet", {})
        live = item.get("liveStreamingDetails", {})

        title = snippet.get("title", "")
        published = snippet.get("publishedAt", "")
        scheduled = live.get("scheduledStartTime", "")
        status = snippet.get("liveBroadcastContent", "none")

        # --- liveStreamingDetails ã‚’ä½¿ã£ã¦å®Ÿéš›ã®é…ä¿¡çŠ¶æ…‹ã‚’å†è©•ä¾¡ ---
        if "actualEndTime" in live:
            status = "completed"
        elif "actualStartTime" in live:
            status = "live"
        elif "scheduledStartTime" in live:
            status = "upcoming"
        
        # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†é¡ï¼ˆstatusã«åˆã‚ã›ã¦ï¼‰ ===
        if "ãƒ•ãƒªãƒ¼ãƒãƒ£ãƒƒãƒˆ" in title or "ãƒ•ãƒªãƒ¼ã‚¹ãƒšãƒ¼ã‚¹" in title:
            section = "freechat"
        elif status == "live":
            section = "live"
        elif status == "upcoming":
            section = "upcoming"
        elif status == "completed":
            section = "completed"
        else:
            section = "uploaded"


        # === ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†é¡ ===
        if "ãƒ•ãƒªãƒ¼ãƒãƒ£ãƒƒãƒˆ" in title or "ãƒ•ãƒªãƒ¼ã‚¹ãƒšãƒ¼ã‚¹" in title:
            section = "freechat"
        elif status == "live":
            section = "live"
        elif status == "upcoming":
            section = "upcoming"
        elif scheduled or live.get("actualEndTime"):
            section = "completed"
        else:
            section = "uploaded"

        videos.append({
            "id": item["id"],
            "title": title,
            "channel": snippet.get("channelTitle", ""),
            "channel_id": snippet.get("channelId", ""),
            "description": snippet.get("description", ""),
            "thumbnail": snippet.get("thumbnails", {}).get("high", {}).get("url", ""),
            "url": f"https://www.youtube.com/watch?v={item['id']}",
            "scheduled": scheduled,
            "published": published,
            "status": status,
            "section": section,
        })
    return videos

# ============================================================== #
# ğŸ”„ ãƒ¡ã‚¤ãƒ³åé›†
# ============================================================== #

def fetch_videos(channel_id, event_type=None, key=None):
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "channelId": channel_id,
        "type": "video",
        "order": "date",
        "maxResults": 50,
        "key": key,
    }

    if event_type in ["live", "upcoming"]:
        params["eventType"] = event_type
        res = requests.get(url, params=params)
        if res.status_code == 403:
            print(f"âš ï¸ 403 Forbidden for {channel_id} ({event_type})")
            return []
        res.raise_for_status()
        data = res.json()

        # ğŸ§© Fallback: çªç™ºãƒ©ã‚¤ãƒ–å¯¾å¿œï¼ˆliveã§0ä»¶ãªã‚‰é€šå¸¸æ¤œç´¢ã¸ï¼‰
        if not data.get("items"):
            print(f"âš ï¸ {channel_id}: no {event_type} items, fallback to normal search")
            del params["eventType"]
            res = requests.get(url, params=params)
            res.raise_for_status()
            data = res.json()

        return [item["id"]["videoId"] for item in data.get("items", []) if "id" in item]

    else:
        params["publishedAfter"] = CUTOFF.strftime("%Y-%m-%dT%H:%M:%SZ")
        res = requests.get(url, params=params)
        if res.status_code == 403:
            print(f"âš ï¸ 403 Forbidden for {channel_id} (normal)")
            return []
        res.raise_for_status()
        return [item["id"]["videoId"] for item in res.json().get("items", []) if "id" in item]


def collect_all():
    cache = load_cache()
    new_data = {"live": [], "upcoming": [], "completed": [], "uploaded": [], "freechat": []}

    for cid, key in CHANNEL_KEYS.items():
        if not key:
            print(f"âš ï¸ Missing API key for {cid}, skipping.")
            continue

        print(f"ğŸ” Checking channel {cid} ...")

        # --- upcoming ---
        upcoming_ids = fetch_videos(cid, "upcoming", key)
        upcoming = fetch_details(upcoming_ids, key)
        new_data["upcoming"].extend(upcoming)

        # --- live ---
        live_ids = fetch_videos(cid, "live", key)
        live = fetch_details(live_ids, key)
        new_data["live"].extend(live)

        # --- é€šå¸¸å‹•ç”»ï¼ˆ6æ™‚é–“ãŠãï¼‰ ---
        now = datetime.utcnow()
        last_update = cache.get("_meta", {}).get(cid)
        if not last_update or (now - datetime.fromisoformat(last_update)) > timedelta(hours=6):
            vid_ids = fetch_videos(cid, None, key)
            vids = fetch_details(vid_ids, key)
            for v in vids:
                if v["section"] in new_data:
                    new_data[v["section"]].append(v)
            cache["_meta"] = cache.get("_meta", {})
            cache["_meta"][cid] = now.isoformat()

    # --- 30æ—¥ä»¥ä¸Šå‰ã‚’å‰Šé™¤ ---
    for k in ["completed", "uploaded"]:
        new_data


