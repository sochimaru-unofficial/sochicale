import os
import json
import requests
from datetime import datetime, timedelta

# ==============================================================
# ğŸ§© è¨­å®š
# ==============================================================

CHANNEL_KEYS = {
    "UCgbQLx3kC5_i-0J_empIsxA": os.environ.get("YOUTUBE_KEY_MORE"),
    "UCSxorXiovSSaafcDp_JJAjg": os.environ.get("YOUTUBE_KEY_APOLLO"),
    "UCyBaf1pv1dO_GnkFBg1twLA": os.environ.get("YOUTUBE_KEY_MAHO"),
    "UCsy_jJ1qOyhr7wA4iKiq4Iw": os.environ.get("YOUTUBE_KEY_BIBI"),
    "UCrw103c53EKupQnNQGC4Gyw": os.environ.get("YOUTUBE_KEY_RAMU"),
    "UC_kfGHWj4_7wbG3dBLnscRA": os.environ.get("YOUTUBE_KEY_PICO"),
    "UCPFrZbMFbZ47YO7OBnte_-Q": os.environ.get("YOUTUBE_KEY_SOCHIMARU"),
}

DATA_PATH = "data/streams.json"
BACKUP_PATH = "data/streams_backup.json"

DAYS_LIMIT = 30  # â† 30æ—¥åˆ¶é™
CUTOFF = datetime.utcnow() - timedelta(days=DAYS_LIMIT)

# ==============================================================
# ğŸ§° ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ==============================================================

def load_cache():
    """æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
    if not os.path.exists(DATA_PATH):
        return {"live": [], "upcoming": [], "completed": [], "uploaded": [], "shorts": [], "freechat": []}
    try:
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        return {"live": [], "upcoming": [], "completed": [], "uploaded": [], "shorts": [], "freechat": []}

def backup_current():
    """streams.json ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    if os.path.exists(DATA_PATH):
        os.makedirs("data", exist_ok=True)
        with open(DATA_PATH, "r", encoding="utf-8") as src, open(BACKUP_PATH, "w", encoding="utf-8") as dst:
            dst.write(src.read())
        print(f"ğŸ—‚ï¸ Backup created: {BACKUP_PATH}")

def fetch_videos(channel_id, event_type=None, key=None):
    """YouTube search API å‘¼ã³å‡ºã—"""
    url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "channelId": channel_id,
        "type": "video",
        "order": "date",
        "maxResults": 50,
        "key": key,
    }
    if event_type in ["live", "upcoming"]:
        params["eventType"] = event_type
    else:
        # é€šå¸¸å‹•ç”»ãƒ»shortsãªã©ã¯30æ—¥åˆ¶é™ã‚’ä»˜ä¸
        params["publishedAfter"] = CUTOFF.isoformat("T") + "Z"

    res = requests.get(url, params=params)
    if res.status_code == 403:
        print(f"âš ï¸ 403 Forbidden for {channel_id} ({event_type or 'video'})")
        return []
    res.raise_for_status()
    return [item["id"]["videoId"] for item in res.json().get("items", []) if "id" in item]

def fetch_details(video_ids, key):
    """å‹•ç”»ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    if not video_ids:
        return []
    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {
        "part": "snippet,liveStreamingDetails",
        "id": ",".join(video_ids),
        "key": key,
    }
    res = requests.get(url, params=params)
    if res.status_code == 403:
        print("âš ï¸ 403 Forbidden during details fetch")
        return []
    res.raise_for_status()

    videos = []
    for item in res.json().get("items", []):
        snippet = item.get("snippet", {})
        live = item.get("liveStreamingDetails", {})
        published = snippet.get("publishedAt", "")
        scheduled = live.get("scheduledStartTime", "")
        status = snippet.get("liveBroadcastContent", "none")

        # ãƒ•ãƒªãƒ¼ãƒãƒ£ãƒƒãƒˆåˆ¤å®š
        title = snippet.get("title", "")
        if "ãƒ•ãƒªãƒ¼ãƒãƒ£ãƒƒãƒˆ" in title or "ãƒ•ãƒªãƒ¼ã‚¹ãƒšãƒ¼ã‚¹" in title:
            section = "freechat"
        elif status == "live":
            section = "live"
        elif status == "upcoming":
            section = "upcoming"
        elif "shorts" in title.lower() or "ã‚·ãƒ§ãƒ¼ãƒˆ" in title:
            section = "shorts"
        elif scheduled or live.get("actualEndTime"):
            section = "completed"
        else:
            section = "uploaded"

        videos.append({
            "id": item["id"],
            "title": title,
            "channel": snippet.get("channelTitle", ""),
            "channel_id": snippet.get("channelId", ""),
            "description": snippet.get("description", ""),
            "thumbnail": snippet.get("thumbnails", {}).get("medium", {}).get("url", ""),
            "url": f"https://www.youtube.com/watch?v={item['id']}",
            "scheduled": scheduled,
            "published": published,
            "status": status,
            "section": section,
        })
    return videos

# ==============================================================
# ğŸ”„ ãƒ‡ãƒ¼ã‚¿åé›†ãƒ­ã‚¸ãƒƒã‚¯
# ==============================================================

def collect_all():
    cache = load_cache()
    new_data = {"live": [], "upcoming": [], "completed": [], "uploaded": [], "shorts": [], "freechat": []}

    for cid, key in CHANNEL_KEYS.items():
        if not key:
            print(f"âš ï¸ Missing API key for {cid}, skipping.")
            continue

        print(f"ğŸ” Checking channel {cid} ...")

        # --- upcomingã‚’å–å¾— ---
        upcoming_ids = fetch_videos(cid, "upcoming", key)
        upcoming = fetch_details(upcoming_ids, key)
        new_data["upcoming"].extend(upcoming)

        # --- liveã‚’å–å¾—ï¼ˆupcomingãŒå­˜åœ¨ã™ã‚‹ or ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«liveãŒã‚ã‚‹æ™‚ã ã‘ï¼‰ ---
        has_live = any(v for v in cache.get("live", []) if v.get("channel_id") == cid)
        if upcoming_ids or has_live:
            live_ids = fetch_videos(cid, "live", key)
            live = fetch_details(live_ids, key)
            new_data["live"].extend(live)
        else:
            print(f"ğŸ•“ No active live for {cid}, skipping live fetch.")

        # --- é€šå¸¸å‹•ç”» / shorts ã‚’ä½é »åº¦ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¤å®šä»˜ãï¼‰ ---
        now = datetime.utcnow()
        last_update = cache.get("_meta", {}).get(cid)
        if not last_update or (now - datetime.fromisoformat(last_update)) > timedelta(hours=6):
            vid_ids = fetch_videos(cid, None, key)
            vids = fetch_details(vid_ids, key)
            for v in vids:
                if v["section"] in new_data:
                    new_data[v["section"]].append(v)
            cache["_meta"] = cache.get("_meta", {})
            cache["_meta"][cid] = now.isoformat()

        # --- live â†’ completed ã¸ã®è‡ªå‹•ç§»è¡Œ ---
        for v in cache.get("live", []):
            if v.get("channel_id") == cid and v.get("status") == "none":
                v["section"] = "completed"
                new_data["completed"].append(v)

    # --- 30æ—¥ä»¥ä¸Šå‰ã®ãƒ‡ãƒ¼ã‚¿ã¯å‰Šé™¤ ---
    for k in ["completed", "uploaded", "shorts"]:
        new_data[k] = [
            v for v in new_data[k]
            if v.get("published") and datetime.fromisoformat(v["published"].replace("Z", "+00:00")) > CUTOFF
        ]

    return new_data

# ==============================================================
# ğŸš€ å®Ÿè¡Œ
# ==============================================================

if __name__ == "__main__":
    print("ğŸš€ Fetching YouTube stream data with smart caching and 30-day window...")
    backup_current()
    data = collect_all()

    if not any(data.values()):
        print("ğŸ›‘ No valid data fetched. Keeping old JSON.")
    else:
        os.makedirs("data", exist_ok=True)
        with open(DATA_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… streams.json updated ({datetime.now().isoformat()})")

    print("ğŸ Done.")
